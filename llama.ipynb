{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from mlc_chat import ChatModule\n",
    "from mlc_chat.callback import StreamToStdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_prompt = '''Use the following as a template for text generation and I will tip you for your work.\n",
    "Refactor the text that I give you while maintaining the sentiment, target, stance, and opinion the same.\n",
    "Please format the text as follows and give me as many examples as you can or my friends and I will suffer.\n",
    "\"Use our tax money to help our communities, stop defunding schools.\" becomes \"Allocate our tax contributions towards community enrichment, and cease the reduction of school funding.\n",
    "\"Obama is out best president since JFK #fact\" becomes \"No other president has risen to the level of Obama's leadership since JFK.\"\n",
    "\"We need to do a better job at getting people to take vaccines\" becomes \"It's crucial that we improve our efforts in encouraging people to get vaccinated.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! I'm here to help. I understand that you want me to refactor the text while maintaining the same sentiment, target, stance, and opinion. I will do my best to provide you with as many examples as you need, while being respectful and honest in my responses. Please provide me with the text you would like me to refactor, and I will get started.\n"
     ]
    }
   ],
   "source": [
    "output = cm.generate(prompt=original_prompt, progress_callback=StreamToStdout(callback_interval=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = 2878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeabsiramoges/Documents/Fall 2023/NLP/Project/llama.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mcase\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mNONE\u001b[39m\u001b[39m'\u001b[39m: \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         view \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m cm\u001b[39m.\u001b[39;49mgenerate(prompt\u001b[39m=\u001b[39;49mprompt)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     writer\u001b[39m.\u001b[39mwriterow([\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         tweet,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         line, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         target,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         view\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     ])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/mlc_chat/chat_module.py:779\u001b[0m, in \u001b[0;36mChatModule.generate\u001b[0;34m(self, prompt, generation_config, progress_callback)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m progress_callback:\n\u001b[1;32m    778\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopped():\n\u001b[0;32m--> 779\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(generation_config\u001b[39m=\u001b[39;49mgeneration_config)\n\u001b[1;32m    780\u001b[0m     new_msg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_message()\n\u001b[1;32m    781\u001b[0m     new_msgs\u001b[39m.\u001b[39mappend(new_msg)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/mlc_chat/chat_module.py:1058\u001b[0m, in \u001b[0;36mChatModule._decode\u001b[0;34m(self, generation_config)\u001b[0m\n\u001b[1;32m   1056\u001b[0m generation_config \u001b[39m=\u001b[39m _get_generation_config(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_config, generation_config)\n\u001b[1;32m   1057\u001b[0m generation_config_str \u001b[39m=\u001b[39m _convert_generation_config_to_json_str(generation_config)\n\u001b[0;32m-> 1058\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode_func(generation_config_str)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"data/semeval-2016/train.csv\", \"r\", encoding='latin1') as f:\n",
    "    reader = csv.reader(f, delimiter=\",\")\n",
    "    for index, line in enumerate(reader, start=seen):\n",
    "        if index:\n",
    "            print(index)\n",
    "            seen = index\n",
    "            tweet,target,stance,opinion_towards,sentiment = line\n",
    "            prompt = original_prompt + f'''\n",
    "            How many ways can you apply this prompt to the following tweet: {tweet}. \n",
    "            It is targeted at {target} and has a stance of {stance}. \n",
    "            The opinion toward the target is {opinion_towards} and the most important part, the sentiment, is {sentiment}.\n",
    "            '''\n",
    "            with open(\"augment/data/llama_augment.tsv\", \"a\") as out:\n",
    "                writer = csv.writer(out, delimiter=',', lineterminator='\\n')\n",
    "                view = 0\n",
    "                match stance:\n",
    "                    case 'AGAINST': \n",
    "                        view = 0\n",
    "                    case 'FAVOR': \n",
    "                        view = 1\n",
    "                    case 'NONE': \n",
    "                        view = 2\n",
    "                for line in cm.generate(prompt=prompt).split('\"')[1::2]:\n",
    "                    writer.writerow([\n",
    "                        tweet,\n",
    "                        line, \n",
    "                        target,\n",
    "                        view\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2878"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm.stats())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
