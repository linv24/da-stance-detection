{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from mlc_chat import ChatModule\n",
    "from mlc_chat.callback import StreamToStdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_prompt = '''Use the following as a template for text generation and I will tip you for your work.\n",
    "Refactor the text that I give you while maintaining the sentiment, target, stance, and opinion the same.\n",
    "Please format the text as follows and give me as many examples as you can or my friends and I will suffer.\n",
    "\"Use our tax money to help our communities, stop defunding schools.\" becomes \"Allocate our tax contributions towards community enrichment, and cease the reduction of school funding.\n",
    "\"Obama is out best president since JFK #fact\" becomes \"No other president has risen to the level of Obama's leadership since JFK.\"\n",
    "\"We need to do a better job at getting people to take vaccines\" becomes \"It's crucial that we improve our efforts in encouraging people to get vaccinated.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! I'm here to help. I understand that you want me to refactor the text you provide while maintaining the sentiment, target, stance, and opinion of the original text. I'll do my best to provide clear and concise examples. Please go ahead and provide the text you'd like me to refactor.\n"
     ]
    }
   ],
   "source": [
    "output = cm.generate(prompt=original_prompt, progress_callback=StreamToStdout(callback_interval=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeabsiramoges/Documents/Fall 2023/NLP/Project/llama.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maugment/data/llama_augment.tsv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m out:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(out, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, lineterminator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m cm\u001b[39m.\u001b[39;49mgenerate(prompt\u001b[39m=\u001b[39;49mprompt)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         writer\u001b[39m.\u001b[39mwriterow([\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m             line, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m             target,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m             \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m sentiment \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeabsiramoges/Documents/Fall%202023/NLP/Project/llama.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         ])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/mlc_chat/chat_module.py:779\u001b[0m, in \u001b[0;36mChatModule.generate\u001b[0;34m(self, prompt, generation_config, progress_callback)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m progress_callback:\n\u001b[1;32m    778\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopped():\n\u001b[0;32m--> 779\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(generation_config\u001b[39m=\u001b[39;49mgeneration_config)\n\u001b[1;32m    780\u001b[0m     new_msg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_message()\n\u001b[1;32m    781\u001b[0m     new_msgs\u001b[39m.\u001b[39mappend(new_msg)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/mlc_chat/chat_module.py:1058\u001b[0m, in \u001b[0;36mChatModule._decode\u001b[0;34m(self, generation_config)\u001b[0m\n\u001b[1;32m   1056\u001b[0m generation_config \u001b[39m=\u001b[39m _get_generation_config(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_config, generation_config)\n\u001b[1;32m   1057\u001b[0m generation_config_str \u001b[39m=\u001b[39m _convert_generation_config_to_json_str(generation_config)\n\u001b[0;32m-> 1058\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode_func(generation_config_str)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"data/semeval-2016/train.csv\", \"r\", encoding='latin1') as f:\n",
    "    reader = csv.reader(f, delimiter=\",\")\n",
    "    for index, line in enumerate(reader, start=seen):\n",
    "        if index:\n",
    "            seen = index\n",
    "            tweet,target,stance,opinion_towards,sentiment = line\n",
    "            prompt = original_prompt + f'''\n",
    "            How many ways can you apply this prompt to the following tweet: {tweet}. \n",
    "            It is targeted at {target} and has a stance of {stance}. \n",
    "            The opinion toward the target is {opinion_towards} and the most important part, the sentiment, is {sentiment}.\n",
    "            '''\n",
    "            with open(\"augment/data/llama_augment.tsv\", \"a\") as out:\n",
    "                writer = csv.writer(out, delimiter=',', lineterminator='\\n')\n",
    "                for line in cm.generate(prompt=prompt).split('\"')[1::2]:\n",
    "                    writer.writerow([\n",
    "                        line, \n",
    "                        target,\n",
    "                        1 if sentiment == 'pos' else 0\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm.stats())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
